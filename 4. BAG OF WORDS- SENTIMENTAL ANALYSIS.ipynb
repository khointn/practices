{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SIMPLE SENTIMENTAL ANALYSIS USING BAG OF WORDS\n",
    "=====\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This is a Sentiment prediction model which I built using Bag of Words algorithm, which aims to simply predict the sentiment of sentences.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset contains 1,600,000 Twitter tweets . The tweets have been annotated (0 = negative, 4 = positive).\n",
    "\n",
    "Link: https://www.kaggle.com/kazanova/sentiment140 \n",
    "\n",
    "## Objective\n",
    "\n",
    "Train a higher accurate model using bigger dataset. Before, I practiced this algorithm by using a 1000 lines dataset which created a very bad model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\Asus\\Downloads\\training.1600000.processed.noemoticon.csv',encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop(['NO_QUERY'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['sentiment','id','sentday','user','sentence']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_list=[]\n",
    "for i in df['sentiment']:\n",
    "    if i==4: #Change the output value from 4 to 1 to closer the range.\n",
    "        sen_list.append(1)\n",
    "    if i==0:\n",
    "        sen_list.append(0)\n",
    "df['sentiment']=sen_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>id</th>\n",
       "      <th>sentday</th>\n",
       "      <th>user</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1587581</th>\n",
       "      <td>1</td>\n",
       "      <td>2190963914</td>\n",
       "      <td>Tue Jun 16 04:11:23 PDT 2009</td>\n",
       "      <td>corinaxjonas</td>\n",
       "      <td>im listening to LVaTT!! its hacking awesome!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136826</th>\n",
       "      <td>1</td>\n",
       "      <td>1976660678</td>\n",
       "      <td>Sat May 30 18:42:22 PDT 2009</td>\n",
       "      <td>Peulo</td>\n",
       "      <td>I think I got 9 hours of sleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1446571</th>\n",
       "      <td>1</td>\n",
       "      <td>2062525203</td>\n",
       "      <td>Sat Jun 06 23:29:15 PDT 2009</td>\n",
       "      <td>chris_cupcake</td>\n",
       "      <td>is in love with Christophe Dubois and his musi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521909</th>\n",
       "      <td>0</td>\n",
       "      <td>2192562189</td>\n",
       "      <td>Tue Jun 16 07:13:14 PDT 2009</td>\n",
       "      <td>RBsmarchinglion</td>\n",
       "      <td>@EmClosk enjoy enjoy times a million. PS. I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289496</th>\n",
       "      <td>1</td>\n",
       "      <td>2002619781</td>\n",
       "      <td>Tue Jun 02 04:43:41 PDT 2009</td>\n",
       "      <td>urvi_rox</td>\n",
       "      <td>Bit soggy after run today. It was only 1/2 hou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268666</th>\n",
       "      <td>1</td>\n",
       "      <td>2000086289</td>\n",
       "      <td>Mon Jun 01 21:23:00 PDT 2009</td>\n",
       "      <td>jossjello</td>\n",
       "      <td>@AudreyLizeth Ahhh!!! Let's go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621781</th>\n",
       "      <td>0</td>\n",
       "      <td>2228942697</td>\n",
       "      <td>Thu Jun 18 15:04:56 PDT 2009</td>\n",
       "      <td>spencercofield</td>\n",
       "      <td>Seriously needs a massage</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>553778</th>\n",
       "      <td>0</td>\n",
       "      <td>2203703506</td>\n",
       "      <td>Wed Jun 17 00:20:17 PDT 2009</td>\n",
       "      <td>RK18</td>\n",
       "      <td>Phew !! Wait... where are my weekends?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1048639</th>\n",
       "      <td>1</td>\n",
       "      <td>1960192061</td>\n",
       "      <td>Fri May 29 07:34:16 PDT 2009</td>\n",
       "      <td>joshuar1313</td>\n",
       "      <td>@aliciadunaway me too baby. im starting my 2nd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1172977</th>\n",
       "      <td>1</td>\n",
       "      <td>1980695580</td>\n",
       "      <td>Sun May 31 07:26:15 PDT 2009</td>\n",
       "      <td>sweetsmile36</td>\n",
       "      <td>getting ready to sleep,after a busy night at w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment          id                       sentday             user  \\\n",
       "1587581          1  2190963914  Tue Jun 16 04:11:23 PDT 2009     corinaxjonas   \n",
       "1136826          1  1976660678  Sat May 30 18:42:22 PDT 2009            Peulo   \n",
       "1446571          1  2062525203  Sat Jun 06 23:29:15 PDT 2009    chris_cupcake   \n",
       "521909           0  2192562189  Tue Jun 16 07:13:14 PDT 2009  RBsmarchinglion   \n",
       "1289496          1  2002619781  Tue Jun 02 04:43:41 PDT 2009         urvi_rox   \n",
       "1268666          1  2000086289  Mon Jun 01 21:23:00 PDT 2009        jossjello   \n",
       "621781           0  2228942697  Thu Jun 18 15:04:56 PDT 2009   spencercofield   \n",
       "553778           0  2203703506  Wed Jun 17 00:20:17 PDT 2009             RK18   \n",
       "1048639          1  1960192061  Fri May 29 07:34:16 PDT 2009      joshuar1313   \n",
       "1172977          1  1980695580  Sun May 31 07:26:15 PDT 2009     sweetsmile36   \n",
       "\n",
       "                                                  sentence  \n",
       "1587581     im listening to LVaTT!! its hacking awesome!!   \n",
       "1136826                   I think I got 9 hours of sleep.   \n",
       "1446571  is in love with Christophe Dubois and his musi...  \n",
       "521909   @EmClosk enjoy enjoy times a million. PS. I ha...  \n",
       "1289496  Bit soggy after run today. It was only 1/2 hou...  \n",
       "1268666                    @AudreyLizeth Ahhh!!! Let's go   \n",
       "621781                          Seriously needs a massage   \n",
       "553778             Phew !! Wait... where are my weekends?   \n",
       "1048639  @aliciadunaway me too baby. im starting my 2nd...  \n",
       "1172977  getting ready to sleep,after a busy night at w...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a new dataset with just only 2 columns 'sentiment' and 'sentence'\n",
    "sentiment=df[['sentiment','sentence']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentiment</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>687559</th>\n",
       "      <td>0</td>\n",
       "      <td>http://thisiswhyyourefat.com/ - who the hell p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302603</th>\n",
       "      <td>1</td>\n",
       "      <td>Yes. I did!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>703436</th>\n",
       "      <td>0</td>\n",
       "      <td>Omg the horse was not even close  ~&amp;lt;3~</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226224</th>\n",
       "      <td>1</td>\n",
       "      <td>@carole_hicks Well I'm coming from downtown Da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470755</th>\n",
       "      <td>0</td>\n",
       "      <td>@super_seb29 Thanks. I don't have any camomile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1068858</th>\n",
       "      <td>1</td>\n",
       "      <td>@amandabree I took the girls to a fabric store...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>295272</th>\n",
       "      <td>0</td>\n",
       "      <td>i just burned the crap out of my hand...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306458</th>\n",
       "      <td>0</td>\n",
       "      <td>Woo! Party at my house! Too bad I'm not there....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135744</th>\n",
       "      <td>1</td>\n",
       "      <td>@MiklBarton seriously! ask @sealabcore he got ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270605</th>\n",
       "      <td>0</td>\n",
       "      <td>well, a dozen grapes for midmorn snack just di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sentiment                                           sentence\n",
       "687559           0  http://thisiswhyyourefat.com/ - who the hell p...\n",
       "1302603          1                                       Yes. I did! \n",
       "703436           0          Omg the horse was not even close  ~&lt;3~\n",
       "1226224          1  @carole_hicks Well I'm coming from downtown Da...\n",
       "470755           0  @super_seb29 Thanks. I don't have any camomile...\n",
       "1068858          1  @amandabree I took the girls to a fabric store...\n",
       "295272           0          i just burned the crap out of my hand... \n",
       "306458           0  Woo! Party at my house! Too bad I'm not there....\n",
       "1135744          1  @MiklBarton seriously! ask @sealabcore he got ...\n",
       "270605           0  well, a dozen grapes for midmorn snack just di..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRE-PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stop words\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aaaw aw \n"
     ]
    }
   ],
   "source": [
    "# Removing special characters and \"trash\"\n",
    "import re\n",
    "def preprocessor(text):\n",
    "     # Remove HTML markup\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    # Save emoticons for later appending\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
    "    # Remove any non-word character and append the emoticons,\n",
    "    # removing the nose character for standarization. Convert to lower case\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower()) + ' ' + ' '.join(emoticons).replace('-', ''))\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Create some random texts for testing the function preprocessor()\n",
    "print(preprocessor('aaaw,Aw'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ei', 'yo', 'solocococo', 'momo']\n",
      "['Hi', ',', 'love', 'hate', 'bate']\n"
     ]
    }
   ],
   "source": [
    "# tokenizer and stemming\n",
    "# tokenizer: to break down our twits in individual words\n",
    "# stemming: reducing a word to its root\n",
    "from nltk.stem import PorterStemmer\n",
    "# Your code here\n",
    "porter = PorterStemmer()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return text.split() \n",
    "\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.stem(word) for word in text.split()]\n",
    "\n",
    "#Test\n",
    "print(tokenizer('Ei yo solocococo momo'))\n",
    "print(tokenizer_porter('Hi , loving hating bating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset in train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['sentence']\n",
    "y = df['sentiment']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREATE MODEL AND TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<function preprocessor at 0x000002F205EF17B8>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_accents=No...\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_porter at 0x000002F20A8D12F0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Pipeline, LogisticRegression, TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tokenizer_porter, preprocessor=preprocessor)\n",
    "\n",
    "clf = Pipeline([('vect', tfidf),('clf', LogisticRegression(random_state=0))])\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.79955\n",
      "confusion matrix:\n",
      " [[94849 25050]\n",
      " [23058 97043]]\n",
      "classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.79      0.80    119899\n",
      "           1       0.79      0.81      0.80    120101\n",
      "\n",
      "    accuracy                           0.80    240000\n",
      "   macro avg       0.80      0.80      0.80    240000\n",
      "weighted avg       0.80      0.80      0.80    240000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Test dataset to evaluate model\n",
    "# classification_report\n",
    "# confusion matrix\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "predictions = clf.predict(X_test)\n",
    "print('accuracy:',accuracy_score(y_test,predictions))\n",
    "print('confusion matrix:\\n',confusion_matrix(y_test,predictions))\n",
    "print('classification report:\\n',classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I used to not believe in religion, but now i am a happy Buddhist --> Negative, Positive = [0.34436768 0.65563232]\n",
      "bad!#!#@@#! --> Negative, Positive = [0.9827388 0.0172612]\n",
      "I 2edwdlove you --> Negative, Positive = [0.11782845 0.88217155]\n"
     ]
    }
   ],
   "source": [
    "twits = [\n",
    "    \"I used to not believe in religion, but now i am a happy Buddhist\",\n",
    "    \"bad!#!#@@#!\",\n",
    "    \"I 2edwdlove you\"\n",
    "]\n",
    "\n",
    "preds = clf.predict_proba(twits)\n",
    "\n",
    "for i in range(len(twits)):\n",
    "    print(f'{twits[i]} --> Negative, Positive = {preds[i]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SAVE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pickle to export the trained model\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(clf, open(os.path.join(r'C:\\Users\\Asus\\Downloads\\Dataset', 'SentimentalPrediction.pkl'), 'wb'), protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'C:\\Users\\Asus\\Downloads\\Dataset\\SentimentalPrediction.pkl', 'rb') as function:\n",
    " function=pickle.load(function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vect',\n",
       "                 TfidfVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
       "                                 preprocessor=<function preprocessor at 0x000002F205EF17B8>,\n",
       "                                 smooth_idf=True, stop_words=None,\n",
       "                                 strip_accents=No...\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=<function tokenizer_porter at 0x000002F20A8D12F0>,\n",
       "                                 use_idf=True, vocabulary=None)),\n",
       "                ('clf',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=0, solver='warn',\n",
       "                                    tol=0.0001, verbose=0, warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
